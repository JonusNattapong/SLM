# Thai Language Model - Pre-trained Fine-tuning Approach

‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ Fine-tune Pre-trained Model ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤

## üéØ Overview

‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡πÉ‡∏´‡∏°‡πà‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ pre-trained models ‡πÅ‡∏•‡∏∞ fine-tune ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏°‡∏≤‡∏Å

## üìÅ New Files

### Core Scripts
- **`finetune_pretrained.py`** - ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö fine-tune pre-trained models
- **`inference_pretrained.py`** - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
- **`upload_pretrained.py`** - ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ Hugging Face

### Utility Scripts  
- **`setup_pretrained.py`** - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á examples
- **`quick_train.py`** - ‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πà‡∏ß‡∏ô‡πÅ‡∏ö‡∏ö interactive

## üöÄ Quick Start

### 1. Setup Environment
```bash
python setup_pretrained.py
```

### 2. Quick Training (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
```bash
python quick_train.py
```

### 3. Manual Training
```bash
# DialoGPT (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö conversation)
python finetune_pretrained.py --model microsoft/DialoGPT-medium --samples 5000 --output_dir ./thai_dialogpt_model

# GPT-2 (general purpose)
python finetune_pretrained.py --model gpt2 --samples 5000 --output_dir ./thai_gpt2_model

# DistilGPT-2 (‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß)
python finetune_pretrained.py --model distilgpt2 --samples 3000 --output_dir ./thai_distilgpt2_model
```

### 4. Test Model
```bash
# Interactive chat
python inference_pretrained.py ./thai_dialogpt_model --mode chat

# Benchmark testing
python inference_pretrained.py ./thai_dialogpt_model --mode benchmark

# Single generation
python inference_pretrained.py ./thai_dialogpt_model --mode generate --prompt "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ"
```

### 5. Upload to Hugging Face
```bash
python upload_pretrained.py ./thai_dialogpt_model YourUsername/thai-dialogpt-v1
```

## üéØ Recommended Models

### 1. microsoft/DialoGPT-medium (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**: ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö conversation, ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ
- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**: ‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤, ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤
- **‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠**: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î

### 2. gpt2 
- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**: balanced ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**: ‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç conversation ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà
- **‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠**: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ general purpose

### 3. distilgpt2
- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**: ‡πÄ‡∏•‡πá‡∏Å, ‡πÄ‡∏£‡πá‡∏ß, ‡πÉ‡∏ä‡πâ RAM ‡∏ô‡πâ‡∏≠‡∏¢
- **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢**: ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤
- **‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠**: ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏î‡πâ‡∏≤‡∏ô hardware

## üîß Features

### LoRA (Low-Rank Adaptation)
- ‡∏•‡∏î memory usage 50-80%
- ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ 2-3 ‡πÄ‡∏ó‡πà‡∏≤
- ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á full fine-tuning
- **‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥**

### Smart Training
- **Auto-detect device**: CUDA/CPU
- **Dynamic batch size**: ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° hardware
- **Gradient accumulation**: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô memory overflow
- **WandB logging**: track ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

### Advanced Inference
- **Interactive chat**: ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö real-time
- **Benchmark mode**: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
- **Customizable generation**: ‡∏õ‡∏£‡∏±‡∏ö temperature, top_p, etc.
- **Thai text optimization**: ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

## üìä Expected Results

### Before (Custom Model)
```
Input: ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ
Output: ‡πå‡∏¥‡∏±‡∏π‡πâ‡πà‡∏∂‡πàs of‡∏µ‡πà ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡πá the and‡∏∏‡∏∑‡πà ‡∏´‡∏£‡∏∏‡πâaing‡∏µ‡πàere B‡πå‡πâ‡πá D S b ‡πÅ‡∏•‡∏∞‡πày‡∏π‡πàes ‡∏ã‡∏¥ andon ‡πÄ‡∏õ
```

### After (Fine-tuned Pre-trained)
```
Input: ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ
Output: ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ 77 ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î ‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 5 ‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà ‡∏†‡∏≤‡∏Ñ‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ ‡∏†‡∏≤‡∏Ñ‡∏Å‡∏•‡∏≤‡∏á ‡∏†‡∏≤‡∏Ñ‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏â‡∏µ‡∏¢‡∏á‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ ‡∏†‡∏≤‡∏Ñ‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ
```

## üí° Why Pre-trained is Better

### 1. **Quality**
- ‡πÇ‡∏°‡πÄ‡∏î‡∏• pre-trained ‡∏£‡∏π‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏•‡πâ‡∏ß
- ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå coherent ‡πÅ‡∏•‡∏∞ meaningful

### 2. **Speed**
- LoRA ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô 3-5 ‡πÄ‡∏ó‡πà‡∏≤
- ‡πÉ‡∏ä‡πâ memory ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 50-80%
- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡∏ö‡∏ô laptop ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÑ‡∏î‡πâ

### 3. **Reliability**
- ‡∏°‡∏µ architecture ‡∏ó‡∏µ‡πà‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå‡πÅ‡∏•‡πâ‡∏ß
- ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ gradient exploding/vanishing
- ‡∏Å‡∏≤‡∏£ converge ‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á

## üîç Training Tips

### Sample Size Guidelines
- **Small test**: 1,000 samples (~30 minutes)
- **Development**: 3,000 samples (~1-2 hours)  
- **Production**: 5,000+ samples (~3-4 hours)

### Hardware Recommendations
- **Minimum**: 8GB RAM + CPU (‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤)
- **Good**: 16GB RAM + GTX 1060 (6GB)
- **Optimal**: 32GB RAM + RTX 3080 (10GB+)

### Memory Optimization
```python
# ‡∏•‡∏î batch size ‡∏ñ‡πâ‡∏≤ out of memory
per_device_train_batch_size=2  # ‡πÅ‡∏ó‡∏ô 4
gradient_accumulation_steps=16  # ‡πÅ‡∏ó‡∏ô 8

# ‡πÉ‡∏ä‡πâ fp16 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU
fp16=True  # ‡∏•‡∏î memory 50%
```

## üìà Monitoring Training

### WandB Integration
```bash
# Set up WandB (optional)
export WANDB_API_KEY="your_key_here"
python finetune_pretrained.py --model gpt2
```

### Local Monitoring
```bash
# ‡∏î‡∏π log file
tail -f logs/training.log

# Monitor GPU usage
nvidia-smi -l 1
```

## üîß Troubleshooting

### Common Issues

#### 1. Out of Memory
```bash
# Solution: Reduce batch size
python finetune_pretrained.py --model distilgpt2 --samples 1000
```

#### 2. Slow Training
```bash
# Solution: Use smaller model or fewer samples
python finetune_pretrained.py --model distilgpt2 --samples 2000
```

#### 3. Poor Quality
```bash
# Solution: Use larger model or more samples
python finetune_pretrained.py --model microsoft/DialoGPT-medium --samples 8000
```

#### 4. Connection Error
```bash
# Check internet connection
ping huggingface.co

# Use cached model if available
export HF_DATASETS_OFFLINE=1
```

## üìù Custom Usage

### Advanced Training Configuration
```python
from finetune_pretrained import ThaiPretrainedFineTuner

fine_tuner = ThaiPretrainedFineTuner(
    model_name="microsoft/DialoGPT-medium",
    output_dir="./my_custom_model",
    use_lora=True
)

# Custom training
fine_tuner.fine_tune(max_samples=10000)
```

### Custom Inference
```python
from inference_pretrained import ThaiFinetunedInference

# Load model
inference = ThaiFinetunedInference("./my_custom_model")

# Generate with custom parameters
result = inference.generate_thai_text(
    "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢",
    max_new_tokens=150,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.2
)
```

## üéØ Next Steps

1. **‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏£‡∏Å**: `python quick_train.py`
2. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û**: `python inference_pretrained.py ./model --mode benchmark`
3. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå**: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô temperature, top_p
4. **‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î**: `python upload_pretrained.py ./model username/model-name`
5. **‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á**: integrate ‡∏Å‡∏±‡∏ö‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô

## üìö Resources

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [Thai NLP Resources](https://github.com/PyThaiNLP/pythainlp)

---

**Note**: ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏´‡∏°‡πà‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
