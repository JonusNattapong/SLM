# Thai SLM MoE Configuration File
# Modify these parameters to customize your model training

[model]
# Model architecture parameters
vocab_size = 30000
hidden_size = 768
num_hidden_layers = 12
num_attention_heads = 12
intermediate_size = 3072
max_position_embeddings = 1024

# MoE specific parameters
num_experts = 8
num_experts_per_token = 2
expert_capacity_factor = 1.25

# Regularization
hidden_dropout_prob = 0.1
attention_dropout_prob = 0.1
layer_norm_eps = 1e-12
initializer_range = 0.02

# Loss weights
aux_loss_alpha = 0.01
router_z_loss_alpha = 0.001

# Special tokens
pad_token_id = 0
bos_token_id = 1
eos_token_id = 2

[training]
# Training hyperparameters
num_epochs = 3
batch_size = 4
gradient_accumulation_steps = 8
learning_rate = 1e-4
weight_decay = 0.1
max_grad_norm = 1.0

# Learning rate schedule
warmup_steps = 1000
scheduler_type = "cosine"

# Optimizer
optimizer_type = "adamw"
adam_beta1 = 0.9
adam_beta2 = 0.95
adam_epsilon = 1e-8

# Logging and saving
logging_steps = 100
save_steps = 500
eval_steps = 250
save_total_limit = 3

# Mixed precision
fp16 = true
fp16_opt_level = "O1"

[dataset]
# Dataset parameters
dataset_name = "ZombitX64/Wikipedia-Thai"
max_samples = 10000  # Set to -1 for full dataset
max_length = 512
stride = 256
train_test_split = 0.9

# Tokenizer parameters
tokenizer_vocab_size = 30000
min_frequency = 2
add_prefix_space = false

[generation]
# Default generation parameters
max_new_tokens = 150
temperature = 0.8
top_k = 50
top_p = 0.9
do_sample = true
repetition_penalty = 1.1

[wandb]
# Weights & Biases configuration
enabled = true
project = "thai-slm-moe"
entity = ""  # Your wandb username/organization
tags = ["thai", "slm", "moe", "language-model"]

[huggingface]
# Hugging Face Hub configuration
organization = ""  # Your HF organization (optional)
model_name = "thai-slm-moe"
private = false
license = "apache-2.0"

[hardware]
# Hardware configuration
device = "auto"  # "auto", "cuda", "cpu"
num_workers = 2
pin_memory = true
use_multiple_gpus = false

[paths]
# File paths
output_dir = "./thai_slm_moe_model"
tokenizer_dir = "./thai_tokenizer"
data_dir = "./data"
cache_dir = "./cache"
log_dir = "./logs"

[evaluation]
# Evaluation parameters
eval_batch_size = 8
eval_max_samples = 100
eval_perplexity = true
eval_generation_quality = true
eval_thai_understanding = true

[web_interface]
# Gradio web interface
port = 7860
share = false
server_name = "127.0.0.1"
enable_queue = true
max_size = 20
